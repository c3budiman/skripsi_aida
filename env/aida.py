# -*- coding: utf-8 -*-
"""SkripsiSentimen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12ZUlVg7TLQaaZpeRu6hXNsSTRf1rcQ0p

##Mount Google Drive
"""

from google.colab import drive
drive.mount('/content/gdrive')
#pertama jalanin sel ini dulu, terus nanti minta authorization code, klik aja itu linknya.

"""##Twitter API"""

import tweepy
from tweepy import OAuthHandler
import json
import pandas as pd

CONSUMER_KEY = "ZU9m0tAFvhHfh8LySOeUDxFCD"
CONSUMER_SECRET = "4i4rulXgdA7WXuhoj6moIQtz95S3DwkGiXzyVBCagM8quNzrmF"
ACCESS_TOKEN = "113829125-VEhtcBGMoi8ZUItOjihIDbDuFePLtrTSRYlIzRs0"
ACCESS_TOKEN_SECRET = "jCIx2GPXqzdemZlja5OvVLDIq5ircLUKw4An1eDHHPM0b"

tweet_fulltext = []
searchquery = 'mrt kotor'
for tweets in tweepy.Cursor(api.search, q=searchquery, count=5000, lang='id', tweet_mode='extended').items():
  if tweets.full_text not in tweet_fulltext:
    tweet_fulltext.append(tweets.full_text) #yang ini masukin tweetsnya ke array tweet_fulltext

df = pd.DataFrame()
df['tweets'] = tweet_fulltext
print(df.shape)
df

auth = OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)
auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)
api = tweepy.API(auth, wait_on_rate_limit=True)

df = pd.DataFrame()
df['tweets'] = tweet_fulltext
print(df.shape)
df.head()

df.to_csv('data/mrtkotor.csv')

!mv mrtkotor.csv /content/gdrive/My\ Drive/skripsiAida/baru/mrtkotor.csv

"""##PREPROCESSING"""

import pandas as pd
import re

#drop symbol,RT,@,http, in tweets
df = pd.read_csv('/content/gdrive/My Drive/skripsiAida/cobaterus/gabungan.csv',header=None)

df[0]=df[0].str.replace('(https?://[\w\.\/]*)', '') #http
df[0]=df[0].str.replace('(?:&(?:lt|nbsp|amp|gt);)', '') #lt,nbsp,gt,amp
df[0]=df[0].str.replace('(@|#)\w+', '') #@ dan #

df[0]=df[0].str.replace('[^A-Za-z0-9\s\-\/]', '') #selain huruf, spasi dan strip
df[0]=df[0].str.replace('(\-|\/)', ' ') #-
df[0]=df[0].str.replace('\n', ' ') # enter
df[0]=df[0].str.replace('\s{2,}', ' ') # spasi lebih dari 2df[0]=df[0].str.replace('(https?://[\w\.\/]*)', '') #http

df.head(100)

df.to_csv('preprogabungan.csv')

!mv preprogabungan.csv /content/gdrive/My\ Drive/skripsiAida/cobaterus/preprogabungan.csv

"""##LOWERCASE"""

import pandas as pd
import re

#drop simbol,RT,@,http, in tweets
df = pd.read_csv('gdrive/My Drive/skripsiAida/cobaterus/preprogabungan.csv',header=None)

df.head()

df[1]=df[1].str.lower()

df.head(100)

df.to_csv('vimgabungan.csv')

!mv vimgabungan.csv /content/gdrive/My\ Drive/skripsiAida/cobaterus/vimgabungan.csv

"""##STEMMING WITH SASTRAWI"""

!pip install sastrawi

import numpy as np
import pandas as pd
import re
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
from tqdm import tqdm_notebook as tqdm

df = pd.read_csv('gdrive/My Drive/skripsiAida/hahahihi.csv',header=None ,names=["tweets"])

stopword = StopWordRemoverFactory().create_stop_word_remover()
stemmer = StemmerFactory().create_stemmer()

def clean_text(data):
  data = re.sub('[^a-zA-Z]',' ', str(data).lower())
  data = re.sub('\byok\b |\byuk\b', 'ayo', data)
  data = re.sub('\bmager\b', 'males', data)
  data = re.sub('\bmalas\b', 'males', data)
  data = re.sub('\bmls\b', 'males', data)
  data = re.sub('\bkuy\b', 'yuk', data)
  data = re.sub('\borg\b', 'orang', data)
  data = re.sub('\bjg\b', 'juga', data)
  data = re.sub('\budh\b', 'sudah', data)
  data = re.sub('\bmangat\b', 'semangat', data)
  data = re.sub('\bcemungut\b', 'semangat', data)
  data = re.sub('\bgas\b', 'yuk', data)
  data = re.sub('\benakeun\b', 'enak', data)
  data = re.sub('\bnaek\b', 'naik', data)
  data = re.sub('\bmmg\b', 'memang', data)
  data = re.sub('\bga\b', 'engga', data)
  data = re.sub('\bengga\b', 'tidak', data)
  data = re.sub('\bttg\b', 'tentang', data)
  data = re.sub('\brush hour\b', 'jam sibuk', data)
  data = re.sub('\bku\b', 'aku', data)
  data = re.sub('\bgak\b', 'tidak', data)
  data = re.sub('\bdgn\b', 'dengan', data)
  data = re.sub('\bbailk\b', 'pulang', data)
  data = re.sub('\bgatau\b', 'tidak tahu', data)
  data = re.sub('\bbat\b', 'banget', data)
  data = re.sub('\bampe\b', 'sampai', data)
  data = re.sub('\blg\b', 'sedang', data)
  data = re.sub('\banjay\b', 'asik', data)
  data = re.sub('\banjg\b', 'anjing', data)
  data = re.sub('\banjiing\b', 'anjing', data)
  data = re.sub('\bantum\b', 'kamu', data)
  data = re.sub('\basiq\b |\basyique\b |\basik\b', 'asyik', data)
  data = re.sub('\bbgt\b |\bbanget\b |\bbanged\b', 'sangat', data)
  data = re.sub('\bribet\b', 'repot', data)




  data = data.split()
  data = ' '.join(data)

  #setelah ngeganti baru ilangin stopword dan imbuhan kata dibawah ini
  #sastrawi remove stopwords
  data = stopword.remove(data) #stopword nya udah di di provide sastrawi
  #sastrawi stemming
  data = stemmer.stem(data)

  return data

df.head()

df['clean_text'] = [clean_text(n) for n in tqdm(df['tweets'])]

not_null_row = [i for i in df.index if len(df.loc[i]['clean_text']) > 0]
df = df.loc[not_null_row]
print(df.shape)
df.head()

df.loc[88:91,'clean_text']

print(df.shape)
df.head()

df.to_csv('dataset_mrt_stem2.csv')

!mv dataset_mrt_stem2.csv /content/gdrive/My\ Drive/skripsiAida/dataset_mrt_stem2.csv

"""##LABELLING"""

df = pd.read_csv('gdrive/My Drive/skripsiAida/dataset_mrt_stem2.csv', index_col = 0)

df.head()

# load positive word
positive = pd.read_csv('/content/gdrive/My Drive/skripsiAida/positive-words.txt', header=None)
positive = positive[0].values.tolist()
positive = '|'.join(positive)
# load negative word
negative = pd.read_csv('/content/gdrive/My Drive/skripsiAida/negative-words.txt', header=None)
negative = negative[0].values.tolist()
negative = '|'.join(negative)

df['positive'] = [len(re.findall(positive, i.lower())) / len(i.split()) for i in df.clean_text]
df['negative'] = [len(re.findall(negative, i.lower())) / len(i.split()) for i in df.clean_text]

df['sentimen'] = ['positive' if df.iloc[i].negative == 0 or df.iloc[i].positive >= 1 else 'netral' if df.iloc[i].positive == df.iloc[i].negative else 'negative' for i in range(df.shape[0])]

df.head()

print('jumlah tweet positif : ', df[(df.sentimen == 'positive')].shape[0])
print('jumlah tweet negatif : ', df[(df.sentimen == 'negative')].shape[0])
#print('jumlah tweet netral : ', df[(df.sentimen == 'netral')].shape[0])

import matplotlib.pyplot as plt

total_negative_tweets = df[(df.sentimen == 'negative')].shape[0]
total_neutral_tweets = df[(df.sentimen == 'netral')].shape[0]
total_positive_tweets = df[(df.sentimen == 'positive')].shape[0] + total_neutral_tweets


df_bar = pd.Series(data=[total_negative_tweets, total_positive_tweets],
               index=['Negative ' + str(total_negative_tweets),
                      'Positive ' + str(total_positive_tweets)])

barnya = plt.bar(df_bar.index, df_bar.values)
barnya[0].set_color('#ff0000')
barnya[1].set_color('#0000ff')
plt.show()

df.to_csv('labelling_mrt_stem.csv')

!mv labelling_mrt_stem.csv /content/gdrive/My\ Drive/skripsiAida/labelling_mrt_stem.csv

"""##BAG OF WORD MODEL"""

df = pd.read_csv('gdrive/My Drive/skripsiAida/labelling_mrt_stem.csv')

df['numbered_sentimen'] = [1 if i == 'positive' else 0 for i in df['sentimen']]

df.head()

bow_model = CountVectorizer(ngram_range=(1,1), min_df=2, binary=True)
# bow_model = TfidfVectorizer(ngram_range=(1,1), min_df=2)
bow_model.fit(df.clean_text)
df_vect = pd.DataFrame(bow_model.transform(df.clean_text).toarray(), columns=bow_model.get_feature_names())
print(df_vect.shape)
df_vect.head()

print(bow_model.get_feature_names())

"""##POSITIF AND NEGATIVE - VECTOR"""

df_vect['rate_positive_word'] = [i for i in df['positive']]
df_vect['rate_negative_word'] = [i for i in df['negative']]
df_vect.head()

"""##TRAINING"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(df_vect, df.numbered_sentimen, test_size=0.2,random_state=123)

model = BernoulliNB()
classifier = model.fit(x_train,y_train)
predict = classifier.predict(x_test)

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(predict,y_test)
cm

accuracy = cm.trace()/cm.sum()
print (accuracy)

"""##GRAFIK"""

df.head()

df_pred = pd.DataFrame(list(df.clean_text), columns = ['tweets'])

df_pred['clean_text'] = [clean_text(n) for n in tqdm(df_pred.tweets)]

not_null_row = [i for i in df_pred.index if len(df_pred.loc[i]['clean_text']) > 0]
df_pred = df_pred.loc[not_null_row]
print(df_pred.shape)
df_pred.head()

df_pred_vect = pd.DataFrame(bow_model.transform(df_pred['clean_text']).toarray(), columns=bow_model.get_feature_names())
df_pred_vect['rate_positive_word'] = [len(re.findall(positive, i.lower())) / len(i.split()) for i in df_pred.clean_text]
df_pred_vect['rate_negative_word'] = [len(re.findall(negative, i.lower())) / len(i.split()) for i in df_pred.clean_text]
df_pred_vect.head()

result = model.predict(df_pred_vect)

print('jumlah positif : ', len([n for n in result if n == 1]))
print('jumlah negatif : ', len([n for n in result if n == 0]))

total_negative_tweets = len([n for n in result if n == 0])
total_positive_tweets = len([n for n in result if n == 1])
total_tweets = total_negative_tweets + total_positive_tweets


df_bar = pd.Series(data=[total_negative_tweets, total_positive_tweets],
               index=['Negative ' + str(total_negative_tweets),
                      'Positive ' + str(total_positive_tweets)])

barnya = plt.bar(df_bar.index, df_bar.values)
barnya[0].set_color('#ff0000')
barnya[1].set_color('#0000ff')
plt.show()

fig1, ax1 = plt.subplots()
ax1.pie([total_negative_tweets/total_tweets, total_positive_tweets/total_tweets],
        explode=[0, 0.1], labels=['negative', 'positive'], autopct='%1.1f%%',
        colors=['#ff0000', '#0000ff'], shadow=True, startangle=90)
ax1.axis('equal')

!pip install wordcloud

from wordcloud import WordCloud, STOPWORDS
from PIL import Image
import urllib
import requests
import numpy as np

df_bow = pd.DataFrame(df_vect)
try:
  del df_bow['rate_positive_word']
  del df_bow['rate_negative_word']
except:
  pass

text = []
dict_text = {}

for x in df_bow.columns:
  text += [x] * sum(df_bow[x])

print(text)
text = '  '.join(text)

for x in df_bow.columns:
  dict_text[x] = (sum(df_bow[x]) / len(text)) / 100

mask = np.array(Image.open(requests.get('https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Disk_pack1.svg/1200px-Disk_pack1.svg.png', stream=True).raw))

wordcloud = WordCloud(background_color='white', collocations=False, mask=mask, contour_width=3, contour_color='firebrick').generate(text)

# plt.imshow(wordcloud, interpolation='bilinear')
# plt.axis("off")

plt.figure(figsize=[20,10])
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.tight_layout(pad=0)
plt.show()

"""##PREDICT"""

data_predict = [
    'Pengin naik kereta mrt',
    'akhirnya mrt tiap 5 menit sekali pas rush hour',
    'mrt memperbolehkan buka puasa didalam gerbong',
    'akhirnya mrt pun menjadi alternatif daripada tj cepat dan nyaman ga turun turun',
    'akhirnya hari ini nyobain mrt jakarta wkwk ketinggalan banget ya baru nyobain sekarang pertama kali naik mrt itu di singapore dan ternyata mrt jakarta sama bagusnya kok sama di singapore',
    'akhirnya merasakan juga naik mrt senyum senyum bangga terus dari tadi bagus',
    'aku kira akan desek2an ternyata nggak sama sekali seneng banget sama mrt menjelang jam buka puasa gini',
    'alhasil balik ke cipete naik mrt laper bat sumpa makan mcd terus kenyang dan kemudian bego setelah itu mau gimana',
    'anies disuruh mundur karena banjir di jakarta yang cuma mengakibatkan arus pengungsian lebih rendah dibandingkan banjir tahun 2014 2016 tentu banjir di jalan ibukota yang meluas akibat kelalaian proyek lrt mrt',
    'anjing krl gasekalian mrt biar lebih kerenan apa',
    'apa sih pengaruhnya mrt untuk perkembangan budaya di jakarta'


]

df_pred = pd.DataFrame(data_predict, columns = ['tweets'])

df_pred.head()

df_pred['clean_text'] = [clean_text(n) for n in tqdm(df_pred.tweets)]

not_null_row = [i for i in df_pred.index if len(df_pred.loc[i]['clean_text']) > 0]
df_pred = df_pred.loc[not_null_row]
print(df_pred.shape)
df_pred.head()

df_pred_vect = pd.DataFrame(bow_model.transform(df_pred['clean_text']).toarray(), columns=bow_model.get_feature_names())
df_pred_vect['rate_positive_word'] = [len(re.findall(positive, i.lower())) / len(i.split()) for i in df_pred.clean_text]
df_pred_vect['rate_negative_word'] = [len(re.findall(negative, i.lower())) / len(i.split()) for i in df_pred.clean_text]
df_pred_vect.head()

model.predict(df_pred_vect)
